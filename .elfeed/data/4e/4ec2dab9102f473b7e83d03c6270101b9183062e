<!-- SC_OFF --><div class="md"><p>I&#39;ve got a big mdadm array of spinning rust (101TB). There is currently a 4TB SSD cache, handled by using bcache. This helps keep thrashing down, reads rarely hits the spinners, noise/power usage is down, and generally offers a big plus to the system. It works quite well for what I use it for, but I have a reservation.</p> <p>I don&#39;t like is being tied to bcache. I don&#39;t have a EXT4 file system that is easily usable (I know I can loopback with an offset) if things go south because of bcache. There was a bcache-related error last year, that I recovered from, but it was a scary few days. </p> <p>I am aware that ZFS offers L2ARC, but I need to be able to expand the array (yes I know it&#39;s coming <em>someday soon</em>Â®). LVM also has a lvmcache, but gives me the same reservation as bcache. bcachefs exists, seems to still be in alpha. I just don&#39;t like the idea of adding a <em>required</em> layer between mdadm and the filesystem.</p> <p>I need mdadm. I need a partition. I need a filesystem. I don&#39;t need something adding extra complexity: something else that can break. This has been sitting in my head for a while now.</p> <p>On a totally unrelated not, I stumbled across cachefilesd&#39;s use of fs-cache when trying to make my seedbox run smoother. It does a very good job of caching the NFS-mounted file system and keeping it from bothering the NAS when seeding my linux isos. I thought up a potentially Rube Goldberg idea, and I was wondering if calmer, more experienced heads could tell me <del>if I am insane</del>how insane I am.</p> <ol> <li>Reformat my array as straight EXT4 on top of mdadm.</li> <li>Mount the resulting filesystem in <code>/raw-data</code>, which is not the intended directory. </li> <li>Export <code>/raw-data</code> through NFS, <code>no_root_squash</code> to localhost.</li> <li>Mount the NFS-share in <code>/data</code>, the place where I really want the files. </li> <li>Now I spin up <code>cachefilesd</code> and give it the 4TB bcache is currently using.</li> </ol> <p>Pros</p> <ul> <li>SSD caching of the file system</li> <li>Caching can be bypassed</li> <li>No added zfs/bcache/lvm layer</li> <li>seedbox can access the <code>/notcacheddata</code>, so the main cache doesn&#39;t waste time on what only the seedbox wants</li> </ul> <p>Cons</p> <ul> <li>NFS overhead. I expect this to be very low impact. The NAS is an i7-11700 with no other job anyway.</li> <li>fs-cache overhead. Again, I don&#39;t expect this to do much. I&#39;ve already made the decision to spend cpu cycles on SSD caching, so it matters little if I use bcache or fs-cache.</li> <li>?????</li> </ul> <p>This also seems to follow the &quot;do one thing and do it well&quot; ideology. I have mdadm handle raid. I make a partition. mkfs.EXT4 gives me a filesystem. NFS handles file sharing. fs-cache takes care of caching. If something goes screwy, NFS and fs-cache are optional parts; I have a EXT4 filesystem &amp; mdadm to work with. No added layers anywhere.</p> <p>Is this totally stupid?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/gamblodar"> /u/gamblodar </a> <br/> <span><a href="https://www.reddit.com/r/linux/comments/scmlbq/ssd_caching_bcache_alternative/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/linux/comments/scmlbq/ssd_caching_bcache_alternative/">[comments]</a></span>